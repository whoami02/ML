{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/whoami02/ML/blob/master/LLM%20on%20HP%20book.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q python-dotenv langchain transformers PyPDF2 weaviate-client sentence-transformers accelerate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdl1Huvch8N0",
        "outputId": "d7a9109d-8b20-49a3-cc8b-e99f700f1fab"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m808.6/808.6 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.3/120.3 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.2/188.2 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.3/215.3 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "azXXNv7OhfXo"
      },
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "# from langchain import OpenAI\n",
        "from langchain import HuggingFaceHub, LLMChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSmA0wl9hfXp",
        "outputId": "3f2f5f2a-763c-4172-a3c0-9229764bfccd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hrGGVxhGhfXp",
        "outputId": "de1e4d6b-9cf4-4335-9240-777ba46d60f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:127: FutureWarning: '__init__' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
            "  warnings.warn(warning_message, FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "# llm = OpenAI(model='gpt-3.5-turbo-1106' ,temperature=2)\n",
        "# tools = load_tools(['serpapi', 'llm-math'], llm=llm)\n",
        "\n",
        "hub_llm = HuggingFaceHub(repo_id=\"tiiuae/falcon-7b-instruct\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCNkkslohfXq"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables = [\"question\"],\n",
        "    template = \"As a domain expert you have an unparalleled knowledge on the topic, you are the best in the buisness and your word is the law. As an expert, some poor individuals seek some percentage of your greatness to succeed. You will guide and help them excel by answering their queries :{question}\"\n",
        "    # template = \"wkwkwkw: {question}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0nDGjiw2hfXq"
      },
      "outputs": [],
      "source": [
        "hub_chain = LLMChain(prompt=prompt, llm=hub_llm, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_mrebZidhfXq",
        "outputId": "653802f0-0796-4f34-f2d1-5836c7e51db2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mAs a domain expert you have an unparalleled knowledge on the topic, you are the best in the buisness and your word is the law. As an expert, some poor individuals seek some percentage of your greatness to succeed. You will guide and help them excel by answering their queries :What are the steps to deply a machine learning application?\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "1. Define the problem: Understand the problem and its requirements.\n",
            "2. Choose the right tools: Select the right tools and frameworks for the job.\n",
            "3. Gather data: Collect and process data to train the machine learning model.\n",
            "4. Train the model: Train the model using the data and algorithms.\n",
            "5. Evaluate the model: Evaluate the model's performance and refine it if necessary.\n",
            "6. Deploy the model: Deploy the model in production and monitor its performance.\n",
            "7. Maintain the model: Keep the model up to date with new data and trends.\n"
          ]
        }
      ],
      "source": [
        "print(hub_chain.run(\"What are the steps to deply a machine learning application?\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g-otUiPohfXq",
        "outputId": "2abfd9ea-03a6-4440-c484-e4295e35ba9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mAs a domain expert you have an unparalleled knowledge on the topic, you are the best in the buisness and your word is the law. As an expert, some poor individuals seek some percentage of your greatness to succeed. You will guide and help them excel by answering their queries :what are the steps to deploy a machine learning application\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "?\n",
            "1. Define the problem: Understand the problem and its requirements.\n",
            "2. Choose a framework: Select a suitable machine learning framework.\n",
            "3. Data collection: Collect and preprocess the data.\n",
            "4. Feature engineering: Select and clean the features.\n",
            "5. Model selection: Select the appropriate model.\n",
            "6. Model training: Train the model.\n",
            "7. Model evaluation: Evaluate the model.\n",
            "8. Model deployment: Deploy the model.\n",
            "9. Model maintenance: Monitor the model.\n",
            "10. Model adaptation: Adapt the model to changing conditions.\n"
          ]
        }
      ],
      "source": [
        "print(hub_chain.run(\"what are the steps to deploy a machine learning application\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "DoYpnIA6hfXq"
      },
      "outputs": [],
      "source": [
        "from PyPDF2 import  PdfReader\n",
        "from langchain.vectorstores import Weaviate\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    pipeline,\n",
        ")\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "sAd-MV0LhfXq"
      },
      "outputs": [],
      "source": [
        "pdf_reader = PdfReader(r\"/content/HP-6--Harry-Potter-and-the-Half-Blood-Prince.pdf\")\n",
        "book_ = pdf_reader.pages[16:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_bFMJozNhfXr",
        "outputId": "5e4e5a66-60c9-45be-de78-d228f5fc2a8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 655/655 [00:09<00:00, 72.37it/s]\n"
          ]
        }
      ],
      "source": [
        "text = []\n",
        "for i in tqdm(range(0,len(pdf_reader.pages)-16)):\n",
        "    text.append(book_[i].extract_text())\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7ZcAQqhhfXr",
        "outputId": "702c50f1-2b45-46ac-85c7-01980b320f70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 655/655 [00:00<00:00, 3225.88it/s]\n"
          ]
        }
      ],
      "source": [
        "corpus = ''\n",
        "for i in tqdm(range(len(text))):\n",
        "    temp = text[i]\n",
        "    temp = re.sub(r\"(@\\[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", temp)\n",
        "    temp = re.sub(r\" +\", \" \", temp)\n",
        "    temp = re.sub('C H A P T E R ', '', temp)\n",
        "    corpus += temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "jc8zdz12hfXr",
        "outputId": "2c94dcca-21a9-4a87-f11e-f4589c4bc1a9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'THE OTHER MINISTER 3 the dark glass He knew that cough He had heard it before He turned very slowly to face the empty room Hello he said trying to sound braver than he felt For a brief moment he allowed hi mself the impossible hope that nobody would answer him However a voice responded at once a crisp decisive voice that sounded as though it were reading a prepared statement It was coming as the Prime Minister had known at the first cough from th e froglike little man wearing a long silver wig w'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "corpus[:500]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "VWHldYchhfXr"
      },
      "outputs": [],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 512,\n",
        "    chunk_overlap  = 32,\n",
        "    length_function = len,\n",
        ")\n",
        "texts = text_splitter.split_text(corpus)\n",
        "texts = text_splitter.create_documents(texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "NfmVtfu4hfXr"
      },
      "outputs": [],
      "source": [
        "import weaviate\n",
        "\n",
        "auth_config = weaviate.AuthApiKey(api_key=\"RPlCu70oXcOsVsWUWf0sjMNlrV5xFU7oatgA\")\n",
        "\n",
        "client = weaviate.Client(\n",
        "  url=\"https://idk-915fsd3w.weaviate.network\",\n",
        "  auth_client_secret=auth_config\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Ahjv5PMbhfXr"
      },
      "outputs": [],
      "source": [
        "# embeddings = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "embedding_model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "model_kwargs = {\"device\": \"cuda\"}\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "  model_name=embedding_model_name,\n",
        "  model_kwargs=model_kwargs\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "zezZQ-njhfXr"
      },
      "outputs": [],
      "source": [
        "docsearch = Weaviate.from_documents(texts, embeddings, client=client, by_text=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XrfgLc5GhfXs",
        "outputId": "f897ab06-a45f-4a13-dffe-9109718dfc0c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='isnt it said Slughorn HORACE SLUGHORN 71 Not really sa id Harry coldly Slughorn looked down at him in surprise You mustnt think Im prejudiced he said No no no Havent I just said your mother was one of my alltime favorite students And there was Dirk Cresswell in the year afte r her too now Head of the Goblin Liaison Office of course another Muggleborn a very gifted student and still gives me excell ent inside information on the goingson at Gringotts He bounced up and down a little smiling in a selfsatisfied'),\n",
              " Document(page_content='Slughorn As they set off down the garden pa th Slughorns voice floated after them Ill want a pay rise Dumbledore Dumbledore chuckled The garden gate swung shut behind them and they set off back down th e hill through the dark and the swirling mist Well done Harry said Dumbledore I didnt do anything said Harry in surprise Oh yes you did You showed Horace exactly how much he stands to gain by returning to Hogwarts Did you like him Er Harry wasnt sure whether he li ked Slughorn or not He supposed he had been'),\n",
              " Document(page_content='could find him The smile slid from Slughorns face as quickly as the blood from his walls CHAPTER FOUR 72 Of course not he said look ing down at Harry I have been out of touch with ev erybody for a year Harry had the impression that the words shocked Slughorn himself he looked quite unsettled for a moment Then he shrugged Still the prudent wizard keep s his head down in such times All very well for Dumbledore to talk but taking up a post at Hogwarts just now would be tantamount to declaring my public'),\n",
              " Document(page_content='he recognized at once It was a much younger Horace Slughorn Harry was so used to him bald that he found the sight of Slughorn with thick shiny strawcolored hair quite disconcertin g it looked as though he had had his head thatched though there was already a shiny Galleonsized bald patch on his crown Hi s mustache less massive than it was these days was gingeryblond He was not quite as rotund as the Slughorn Harry knew though the golden buttons on his richly embroidered waistcoat were taking a fair amount')]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "docsearch.similarity_search(\"tell about horace slughorn\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142,
          "referenced_widgets": [
            "dce9141a3f2144cf9819c11ca3e648e3",
            "775a5073129148e581349eff93935f18",
            "5ab6129dc276448b889b30a44ad63308",
            "0bd7c00400774b15b0d593b1ae05da06",
            "bddb67bab59d42b69607f8140a472974",
            "f0f72f5ef36243d6b19dcf945ce08252",
            "c7020f5e27f44e1a83f46c174769012f",
            "1014481bea944844a99d9dcd541e69dc",
            "74fe35299f024618b703d74a5a1e4868",
            "34def14f3bf04f768b396fa5006a9d96",
            "ee91064a0533435293b3f38a2c4e50d8"
          ]
        },
        "id": "deI0prpfhfXs",
        "outputId": "429fc40f-a84a-4474-d1d4-e9a847c501d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers_modules.tiiuae.falcon-7b-instruct.cf4b3c42ce2fdfe24f753f0f0d179202fea59c99.configuration_falcon:\n",
            "WARNING: You are currently loading Falcon using legacy code contained in the model repository. Falcon has now been fully ported into the Hugging Face transformers library. For the most up-to-date and high-performance version of the Falcon model code, please update to the latest version of transformers and then load the model without the trust_remote_code=True argument.\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dce9141a3f2144cf9819c11ca3e648e3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n"
          ]
        }
      ],
      "source": [
        "model = \"tiiuae/falcon-7b-instruct\" #tiiuae/falcon-40b-instruct\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model, return_lenght=False, return_token_type_ids=False)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model,\n",
        "    torch_dtype = torch.bfloat16,\n",
        "    # quantization_config = BitsAndBytesConfig(\n",
        "        # load_in_8bit = True,\n",
        "        # bnb_in_8bit_use_double_quant=True,\n",
        "        # bnb_8bit_quant_type = \"nf4\",\n",
        "        # load_in_8bit_fp32_cpu_offload=True,\n",
        "    # ),\n",
        "    device_map=\"auto\",\n",
        "    offload_folder=\"offload\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "pipeline = pipeline(\n",
        "    \"text-generation\", #task\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    trust_remote_code=True,\n",
        "    # offload_folder=\"./offload\",\n",
        "    device_map=\"auto\",\n",
        "    max_length=2056,\n",
        "    do_sample=True,\n",
        "    top_k=10,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "PrLzkLBshfXs"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt_template = \"\"\" Use the following pieces of context to answer the questions at the end. If you don't know the answer, please think rationally and provide the relevant answer from your knowledge base,\n",
        "{context}\n",
        "Question : {question}\n",
        "\"\"\"\n",
        "\n",
        "PROMPT = PromptTemplate(\n",
        "    template=prompt_template, input_variables = [\"context\", \"question\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ihvE3tafhfXt"
      },
      "outputs": [],
      "source": [
        "llm = HuggingFacePipeline(pipeline = pipeline)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "gwKGChelhfXt"
      },
      "outputs": [],
      "source": [
        "chain_type_kwargs = {\"prompt\":PROMPT}\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm, chain_type=\"stuff\", retriever=docsearch.as_retriever(), chain_type_kwargs=chain_type_kwargs\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5jDWsh7hfXt",
        "outputId": "2c6fd18b-c1e2-4955-dd76-529e9d968463"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Hagrid buries Aragog in his backyard in Deathly Hallows.\n"
          ]
        }
      ],
      "source": [
        "response = qa_chain.run(\"Who does Hagrid bury in his backyard?\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P21o3RAl2CDa"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "dce9141a3f2144cf9819c11ca3e648e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_775a5073129148e581349eff93935f18",
              "IPY_MODEL_5ab6129dc276448b889b30a44ad63308",
              "IPY_MODEL_0bd7c00400774b15b0d593b1ae05da06"
            ],
            "layout": "IPY_MODEL_bddb67bab59d42b69607f8140a472974"
          }
        },
        "775a5073129148e581349eff93935f18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f0f72f5ef36243d6b19dcf945ce08252",
            "placeholder": "​",
            "style": "IPY_MODEL_c7020f5e27f44e1a83f46c174769012f",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "5ab6129dc276448b889b30a44ad63308": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1014481bea944844a99d9dcd541e69dc",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_74fe35299f024618b703d74a5a1e4868",
            "value": 2
          }
        },
        "0bd7c00400774b15b0d593b1ae05da06": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_34def14f3bf04f768b396fa5006a9d96",
            "placeholder": "​",
            "style": "IPY_MODEL_ee91064a0533435293b3f38a2c4e50d8",
            "value": " 2/2 [01:10&lt;00:00, 32.64s/it]"
          }
        },
        "bddb67bab59d42b69607f8140a472974": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0f72f5ef36243d6b19dcf945ce08252": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c7020f5e27f44e1a83f46c174769012f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1014481bea944844a99d9dcd541e69dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "74fe35299f024618b703d74a5a1e4868": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "34def14f3bf04f768b396fa5006a9d96": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee91064a0533435293b3f38a2c4e50d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}